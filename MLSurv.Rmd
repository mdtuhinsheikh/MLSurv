---
title: "Survival Analysis: an era into Machine Learning"
subtitle: "Guest Lecture"
author: Tuhin Sheikh, Department of Statistics, UConn
date: ""
output: 
  ioslides_presentation:
    css: style.css
    widescreen: true
bibliography: Bibliography.bib
link-citations: yes
linkcolor: blue
#font-family: 'Helvetica'
#css: custom.css
#output: slidy_presentation
#footer: "UCSAS 2019"
#output: rmarkdown::github_document
#output: revealjs::revealjs_presentation
#mainfont: Times New Roman
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source("need.packages.R")
pkg <- c("kableExtra", "dplyr", "ggplot2", "DT", "httpuv",
         "mlbench", "ipred", "randomForestSRC", "MASS",
          "survival", "kableExtra", "knitr", "xml2")
need.packages(pkg)
```


## Getting Started
- The source codes and necessary documentations are available at the repository: ( https://github.com/mdtuhinsheikh/MLSurv).
- The lecture slides are available at the [https://mdtuhinsheikh.github.io/MLSurv/](https://mdtuhinsheikh.github.io/MLSurv/).


## About Me
- Third year Ph.D. Student in Statistics, UConn.
- Research Interests:
  + Bayesian biostatistics
  + Modelling informative dropout process
  + Machine learning and deep learning
  + Microbiome data analysis
- Future goals: 
  + Contribute to the broader field of biostatistics and statistical learning.

## Assumption about the audience
- Have good understanding of time-to-event data.
- Familiar with the well-known survival analysis methods.
  
## Aims of the Lecture
- By the end of this lecture, the participants should have a basic understanding of
  + the use of machine learning and deep learning techniques
  + possible impacts these techniques can have in the field of survival analysis

## Contents 
- Basics of machine learning and deep learning
- Applications of machine learning in survival analysis
- Recent developments
- Future directions

## Machine Learning
- Humans learn from past experiences, <u>whereas</u>, machines follow instructions given by humans.
- What if human can train the machines? 


## An illustration of Machine Learning
<center>
![](./Figures/machineLearningBasics.png){width=650px, height=300px}
</center>

## Basic paradigm
<center>
![](./Figures/MLbasicParadigm.png){width=750px, height=300px}
</center>

## An example of machine learning
<center>
![](./Figures/getTheCake1.png){width=750px, height=300px}
</center>

## An example of machine learning cont...
<center>
![](./Figures/getTheCake2.png){width=750px, height=200px}
</center>

## An example of machine learning cont...
<center>
![](./Figures/getTheCake3.png){width=750px, height=300px}
</center>

## Basics of Deep Learning
- **Deep Learning** is very similar to human neural system
  + It is a special kind of **Machine Learning** [@goodfellow2016deep].
- Especially, useful to generalize complicated functions in high dimensional space.
- Synonymous terms: 
  + Deep neural network (DNN)
  + Deep feed-forward networks
  + feed-forward neural networks 
  + multi-layer perceptrons (MLPs). 


## Understanding neural system
<center>
![](./Figures/neuroBiology.png){width=850px, height=300px}
</center>

## Motivating example 1 for DNN
<center>
![](./Figures/motivationalExampleOfNeuralNetwork.png){width=850px, height=300px}
</center>

## Motivating example 1 for DNN cont..
<center>
![](./Figures/motivationalExampleOfNeuralNetwork2.png){width=850px, height=300px}
<center>

## Motivating example 1 for DNN cont..
<center>
![](./Figures/motivationalExampleOfNeuralNetwork3.png){width=850px, height=300px}
</center>

## Motivating example 2 for DNN
<center>
![](./Figures/iphoneX.png){width=850px, height=300px}
</center>

## Visualization of one-layer DNN 
<center>
![](./Figures/singleLayerNeuralNetwork.png){width=550px, height=300px}
</center>

## Visualization of two-layer DNN 
<center>
![](./Figures/twoLayerNeuralNetwork.png){width=550px, height=300px}
</center>

## Visualization of multi-layer DNN 
<center>
![](./Figures/threeLayerNeuralNetwork.png){width=850px, height=300px}
</center>

## Applications
- **Data mining:** web click data, medical records, Google search.
- **Signal recognition:** autonomous helicopter, handwriting recognition, voice recognition, Machine translation, Anomaly detection.
- **Self customizing program:** Amazon, Netflix, Bixby (Samsung), Siri (iphone).
- **On survival analysis:**
  + Predicting patients' survival
  + Classifying competing risks for an event
  + Personalized treatment recommender system

## Survival data
- Notations: 
$$
\begin{align*}
i     &: \text{index for subject } (i=1, \ldots, n)\\
T_i^* &: \text{time for event for subject }  i \\
C_i   &: \text{censoring time for subject } i \\
T_i   &= \min (T_i^*, C_i), \text{observed  time for subject } i \\ 
\delta_i &= I(T_i^* \le C_i), \text{censoring indicator for subject } i\\
\mathbf{x}_i &: \text{vector of covariates for subject } i
\end{align*}
$$
- Observed survival outcome: $\{ (T_i, \delta_i), i=1, \ldots, n \}$.

## An illustration of time-to-event data
<center>
![](./Figures/survivalData.png){width=550px, height=300px}
<!--    \caption{An illustration of survival data [Wang2019]} -->
</center>

## Some interesting questions
- Analyzing patient 1, 4, 8, 9 might give us insight on which features contribute to increase the length of survival?
- Patients who survived to 12 months, what is probability that the event will occur after $t$ time?
- A doctor might want to see what is the chance of re-hospitalizing after the patient is discharged?
- Can we predict the sub-types of the event (considering the sub-types are missing) based on learning the training data?

## Traditional approaches to analyse survival data
- **Non-parametric:** Kaplan-Meier, Nelson-Aalen, Life table
  + Useful at population level but not useful at individual level.
- **Semi-parametric:** Cox proportional hazards (PH) model
  + the proportionality assumption and linearity of the log-risk function might not be appropriate in complex data structure.
- **Parametric:** Accelerated failure time model
  + violation of the distribution might compromise with the consistency of the parameter estimates.


## Why Machine Learning algorithms?
- Increasing data size
- Increasing model size
- Increasing accuracy, complexity, and real-world impact

## Applications in healthcare
<center>
![](./Figures/healthcare.png){width=750px, height=500px}
</center>

## Applications in healthcare cont...
- **Event of interest:** Rehospitalization; Disease recurrence; Cancer survival.
- **Outcome:** Likelihood of hospitalization within $t$ days of discharge. 

## Applications in Education
<center>
![](./Figures/education.png){width=750px, height=400px}
</center>

## Applications in education cont...
- **Event of interest:** Student dropout.
- **Outcome:** Likelihood of a student being dropout within $t$ days. 

## Applications in Crowdfunding
<center>
![](./Figures/crowdfunding.png){width=750px, height=400px}
</center>

## Applications in Crowdfunding cont...
- **Event of interest:** Project success
- **Outcome:** Likelihood of a project being successful within $t$ days.


## Machine Learning for Survival Analysis
- Survival Tree: is similar to decision tree which is built by recursive splitting of tree nodes. 
  + Bagging Trees \citep{hothorn2004bagging}
  + Random Forest (RF) \citep{ishwaran2008random}
- Let us demonstrate one example in the following slides.

## An example of Bagging and RF using R
- R Packages required: randomForestSRC, ipred, MASS, Survival
```{r echo=TRUE}
data(breast, package = "randomForestSRC")
breast <- na.omit(breast)
names(breast)[1:10] # Displaying only ten variable names
```
- The **breast** dataset is from **randomForestSRC** R package.
- For more details about the data: visit this [link](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic))

## An example of Bagging and RF using R cont...
- The goal is to classify the status using decission tree
```{r echo=TRUE}
mod1 <- rfsrc(status ~ ., data = breast, nsplit = 10)
mod2 <- bagging(status ~ ., data = breast, coob=TRUE)
res <- as.data.frame(c(mean(mod1$err.rate[, 1], na.rm = TRUE), 
            mod2$err))
colnames(res) <- "Error Rate"
rownames(res) <- c("RSF", "Bagging")
```
- The misclassification error for the two approaches
```{r}
res
```

## Notes on Baggin and RF
- Both are advantageous for classification.
- For example, a new patient with 'status' <u>unknown</u>,
  + these techniques can help for classifying the status. 
- However, cannot predict time-to-event and perform regression analysis.

## Modelling approaches for survival data
- The Cox PH model is 
$$
\begin{equation}
h_i(t | \mathbf{x}_i) = h_0(t) \exp{(\mathbf{x}'_{i} \boldsymbol{\beta})} \label{coxPH}
\end{equation}
$$
where, $\mathbf{x}_i = (x_{i1}, \ldots, x_{ip})$ denotes the vector of covariates and $\boldsymbol{\beta}$ are the corresponding regression coefficients.
- The Cox PH partial likelihood function is given by
$$
\begin{equation*}
pl(\boldsymbol{\beta}) =  \prod_{i=1}^{D} \Big[ \dfrac{\exp(\mathbf{x}_i^T \boldsymbol{\beta})}{\sum_{j \in \mathcal{R}(T_i)} \exp(\mathbf{x}_j^T \boldsymbol{\beta})} \Big]
\end{equation*}
$$
where, $t_1, t_2, \ldots, t_D$ denote the ordered event times and $\mathcal{R}(t_i)$ denotes the risk set at time $t_i$.

## Traditional Cox PH model cont...
- The corresponding the log partial likelihood function is 
$$
\begin{equation}
\ell(\boldsymbol{\beta}) =  \sum_{i=1}^{D} \Big( \mathbf{x}_i^T \boldsymbol{\beta} -  \log \sum_{j \in \mathcal{R}(t_i)} \exp(\mathbf{x}_j^T \boldsymbol{\beta}) \Big) \label{survLogLik}
\end{equation}
$$

## Estimation
- Maximizing the log partial likelihood function, we get the estimates of the model parameters.
- Evaluating the second derivative with respect to the model parameters, we calculate the information matrix.
- Standard error of the parameters are estimated by the inverse of the information matrix.

## An example of RSF and Cox using R 
- R Packages required: randomForestSRC, ipred, MASS, Survival
```{r echo=TRUE}
data(veteran, package = "randomForestSRC")
names(veteran)[1:10] # Displaying only ten variable names
```
- The **veteran** dataset is from **randomForestSRC** R package.
- For more details about the data: visit this [link](https://stat.ethz.ch/R-manual/R-devel/library/survival/html/veteran.html)

## An example of of RSF and Cox using R cont...
```{r echo=TRUE}
mod3 <- coxph(Surv(time, status)~., data=veteran,x=TRUE,y=TRUE)
mod4 <- rfsrc(Surv(time, status) ~ ., data = veteran, ntree = 100)
cindex <- as.data.frame(c(concordance(mod3)$concordance, 
            1-mean(mod4$err.rate, na.rm=T)))
colnames(cindex) <- "C-index"
rownames(cindex) <- c("Cox PH", "RSF")
```
- The C-index for the two models
```{r}
cindex
```

## Machine Learning for Survival Analysis cont..
- Deep learning: use deep hidden layers to extract the output based on the features. 
  + Bayesian deep learning \citep{polson2017deep, ranganath2016deep}
  + $\color{red}{\text{Note:}}$ could be useful for both classification and regression.

## Previous works
- Feed-forward non-linear proportional hazards model \citep{faraggi1995neural}.
  + Extended the linearity assumption of log-risk with the linear combination of the covariates to non-linear relationship. In particular, used logit function with some hyper-parameters.
- Bayesian version of Feed-forward non-linear proportional hazards model \citep{faraggi1997bayesian}.    
  + This model considered normal prior for the parameters and derived the posterior distribution of the parameters.

## Cox Proportional Hazards Model
- Notations: 
$$
\begin{align*}
i     &: \text{index for subject } (i=1, \ldots, n)\\
t_{ij} &: \text{index time for subjet } i \text{ at } jth \text{ occasion } (j=1, \ldots, n_i)\\
Y_i(t_{ij}) &: \text{repeated measurement for subject } i \text{ at time } t_{ij}\\
T_i^* &: \text{time for event for subject }  i \\
C_i   &: \text{censoring time for subject } i \\
T_i   &= \min (T_i^*, C_i), \text{observed  time for subject } i \\ 
\delta_i &= I(T_i^* \le C_i), \text{censoring indicator for subject } i
\end{align*}
$$
- Longitudinal outcome: $Y_{ij}=\{ Y_i(t_{ij})\;, i=1,\ldots,n, \; j=1,2,\ldots,n_i\}.$
- Observed survival outcome: $\{ (T_i, \delta_i), i=1, \ldots, n \}$.




## Cox Non-proportional Neural Network Model
- The $\exp(\mathbf{x}'_{i} \boldsymbol{\beta})$ function in \eqref{coxPH} is replaced by a more general function $g_{\boldsymbol{\theta}}(\mathbf{x})$ to accommodate non-linear relationship
$$
\begin{equation}
h_i(t | \mathbf{x}_i) = h_0(t) \exp{(g_{\boldsymbol{\theta}}(\mathbf{x}))} \label{coxNPH}
\end{equation}
$$
- The likelihood function
$$
\begin{equation*}
pl(\boldsymbol{\theta}) =  \prod_{i=1}^{D} \Big[ \dfrac{\exp(g_{\boldsymbol{\theta}}(\mathbf{x}_i))}{\sum_{j \in \mathcal{R}(T_i)} \exp(g_{\boldsymbol{\theta}}(\mathbf{x}_i))} \Big]
\end{equation*}    
$$

## Cox Non-proportional Neural Network Model cont..
- Instead of optimizing the $\eqref{survLogLik}$, the following loss function is optimized
$$
\begin{equation}
-\dfrac{1}{N_{\delta=1}} \sum_{i=1}^{D} \Big( g_{\boldsymbol{\theta}}(\mathbf{x}_i) -  \log \sum_{j \in \mathcal{R}(t_i)} \exp(g_{\boldsymbol{\theta}}(\mathbf{x}_i)) \Big) + \lambda ||\boldsymbol{\theta}||^2_2 \label{loss}
\end{equation}
$$
where, $N_{\delta=1}$ is the number of patients with an observable event and $\lambda$ is the $\ell_2$ regularization parameter.
- Gradient descent optimization is used to minimize $\eqref{loss}$.



## Details on layer mechanism
<center>
![](./Figures/detail1.png){width=550px, height=300px}
</center>

## Details on layer mechanism cont...
<center>
![](./Figures/detail2.png){width=550px, height=300px}
</center>

## Computing Gradient: Backpropagation
<center>
![](./Figures/detail3.png){width=550px, height=150px}
</center>

## Computing Gradient: Backpropagation cont..
- How does a small change in one weight (e.g.\ $w^{(2)}_1$) affect the final loss $J(\mathbf{w})$)?
$$
\begin{equation*}
\dfrac{\partial J}{\partial w^{(2)}_1} = \dfrac{\partial J}{\partial \hat{y}} \dfrac{\partial \hat{y}}{\partial w^{(2)}_1}      
\end{equation*} 
$$
- How does a small change in one weight (e.g.\ $w^{(1)}_1$) affect the final loss $J(\mathbf{w})$)?
$$
\begin{equation*}
       \dfrac{\partial J}{\partial w^{(1)}_1} = \dfrac{\partial J}{\partial \hat{y}} \dfrac{\partial \hat{y}}{\partial z} \dfrac{\partial z}{\partial w^{(1)}_1}
\end{equation*} 
$$      

## Gradient descent optimization
- Initialize weights randomly $\sim N(0, \sigma^2)$
- Loop until convergence
  + Compute gradient, $\dfrac{\partial J(\mathbf{w})}{\partial \mathbf{w}}$
  + Update weights, $\mathbf{w}^{(t+1)} \leftarrow \mathbf{w}^{(t)} - \eta \dfrac{\partial J(\mathbf{w})}{\partial \mathbf{w}}$, where $\eta$ is called the learning rate. 
- Return weights.

## Complex Loss Function
<center>
![](./Figures/complexLoss.png){width=550px, height=300px}
</center>


## Neural network for survival
<center>
![](./Figures/neuralNetworkCox.png){width=550px, height=300px}
</center>

## Deep neural network for survival
<center>
![](./Figures/DeepNeuralNetworkCox.png){width=550px, height=300px}
</center>

## Performance of DeepSurv
- Evaluation Metric: Concordance (C) Index
- It is a rank order statistic for predictions against true outcomes.
- The index is calculated as the ratio of the concordant pairs to the total comparable pairs.
- Given the comparable instance pair $(i, j)$, with $t_i$ and $t_j$ are the actual observed times and $S(t_i)$ and $S(t_j)$ are the predicted survival times,
  + The pair $(i, j)$ is concordant if $t_i > t_j$ and $S(t_i) > S(t_j)$. 
  + The pair $(i, j)$ is discordant if $t_i > t_j$ and $S(t_i) < S(t_j)$.
- The concordance probability $=Pr(\hat{T}_i < \hat{T}_j | T_i < T_j)$ measures the concordance between the rankings of actual values and predicted values.

## Performance of DeepSurv cont..
```{r, echo=FALSE}
Experiment <- c("Simulated (Linear)", "Simulated (Nonlinear)", "WHAS", "SUPPORT",
                "METABOLIC", "Simulated Treatment", "Rotterdam GBSG")
CPH <- c(0.779, 0.487, 0.816, 0.583, 0.632, 0.517, 0.659)
DeepSurv <- c(0.778, 0.652, 0.867, 0.619, 0.654, 0.575, 0.676)
RSF <- c(0.758, 0.627, 0.893, 0.619, 0.620, 0.550, 0.648)
dt <- data.frame(Experiment, CPH, DeepSurv, RSF)
datatable(dt, rownames = FALSE, autoHideNavigation = T, 
          caption = 'Table 1: Results from Katzman et al. (2018)',
          class = 'cell-border stripe')
```

## Future direction




## Thanks
- Good luck with survival analysis.
- Feel free to contact for further queries: mdtuhin.sheikh@uconn.edu.

## References



